{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "\n",
    "path.append(r'../../src')\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import tqdm\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import networkx.algorithms as nxalg\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from data_utils import SymbolsManager\n",
    "from data_utils import convert_to_tree\n",
    "from collections import OrderedDict\n",
    "from pythonds.basic.stack import Stack\n",
    "\n",
    "## Some options about constituency tree construction :\n",
    "# * whether cut root node\n",
    "# * whether cut line/pos node\n",
    "# * whether link word nodes\n",
    "# * whether split sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x269892e48b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cut_root_node = True\n",
    "_cut_line_node = True\n",
    "_cut_pos_node = False\n",
    "_link_word_nodes = False\n",
    "_split_sentence = False\n",
    "source_data_dir = r\"G:\\My Drive\\research\\Graph2Tree\\data\\TextData\" + \"\\\\\"\n",
    "output_data_dir = r\"G:\\My Drive\\research\\Graph2Tree\\data\\GraphConstruction\\\\\"+\"\\\\\"\n",
    "batch_size = 30\n",
    "min_freq = 2\n",
    "max_vocab_size = 15000\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputPreprocessor(object):\n",
    "    def __init__(self, url='http://localhost:9000'):\n",
    "        self.nlp = StanfordCoreNLP(url)\n",
    "\n",
    "    def featureExtract(self, src_text, whiteSpace=True):\n",
    "        if src_text.strip() in preparsed_file.keys():\n",
    "            return preparsed_file[src_text.strip()]\n",
    "        print(\"miss!\")\n",
    "        data = {}\n",
    "        output = self.nlp.annotate(src_text.strip(), properties={\n",
    "            'annotators': \"tokenize,ssplit,pos,parse\",\n",
    "            \"tokenize.options\": \"splitHyphenated=true,normalizeParentheses=false\",\n",
    "            \"tokenize.whitespace\": whiteSpace,\n",
    "            'ssplit.isOneSentence': True,\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "        output = json.loads(output)\n",
    "        snt = output['sentences'][0][\"tokens\"]\n",
    "        depency = output['sentences'][0][\"basicDependencies\"]\n",
    "        data[\"tok\"] = []\n",
    "        data[\"pos\"] = []\n",
    "        data[\"dep\"] = []\n",
    "        data[\"governor\"] = []\n",
    "        data[\"dependent\"] = []\n",
    "        data['parse'] = output['sentences'][0]['parse']\n",
    "        for snt_tok in snt:\n",
    "            data[\"tok\"].append(snt_tok['word'])\n",
    "            data[\"pos\"].append(snt_tok['pos'])\n",
    "        for deps in depency:\n",
    "            data[\"dep\"].append(deps['dep'])\n",
    "            data[\"governor\"].append(deps['governor'])\n",
    "            data[\"dependent\"].append(deps['dependent'])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preparsed_file():\n",
    "    if not os.path.exists(\"./file_for_parsing_aqua.pkl\"):\n",
    "        file_for_parsing = {}\n",
    "        processor_tmp = InputPreprocessor()\n",
    "\n",
    "        with open(source_data_dir + \"all_aqua.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            for l in tqdm.tqdm(lines):\n",
    "                str_ = l.strip().split('\\t')[0]\n",
    "                file_for_parsing[str_] = processor_tmp.featureExtract(str_)\n",
    "\n",
    "        pkl.dump(file_for_parsing, open(\"./file_for_parsing_aqua.pkl\", \"wb\"))\n",
    "    else:\n",
    "        preparsed_file = pkl.load(open(\"./file_for_parsing_aqua.pkl\", \"rb\"))\n",
    "    return preparsed_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparsed_file = get_preparsed_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preparsed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node():\n",
    "    def __init__(self, word, type_, id_):\n",
    "        # word: this node's text\n",
    "        self.word = word\n",
    "        # type=0 for word nodes, 1 for constituency nodes, 2 for dependency nodes(if they exist)\n",
    "        self.type = type_\n",
    "\n",
    "        # id: unique identifier for every node\n",
    "        self.id = id_\n",
    "\n",
    "        self.head = False\n",
    "\n",
    "        self.tail = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "\n",
    "\n",
    "def split_str(string):\n",
    "    if \" . \" not in string:\n",
    "        return [string]\n",
    "    else:\n",
    "        s_arr = string.split(\" . \")\n",
    "        res = []\n",
    "        for s in s_arr:\n",
    "            if s[-1] != \".\" and s != s_arr[-1]:\n",
    "                s = s + \" .\"\n",
    "            res.append(s)\n",
    "        return res\n",
    "\n",
    "\n",
    "def cut_root_node(con_string):\n",
    "    tmp = con_string\n",
    "    if con_string[0] == '(' and con_string[-1] == ')':\n",
    "        tmp = con_string[1:-1].replace(\"ROOT\", \"\")\n",
    "        if tmp[0] == '\\n':\n",
    "            tmp = tmp[1:]\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def cut_pos_node(g):\n",
    "    node_arr = list(g.nodes())\n",
    "    del_arr = []\n",
    "    for n in node_arr:\n",
    "        edge_arr = list(g.edges())\n",
    "        cnt_in = 0\n",
    "        cnt_out = 0\n",
    "        for e in edge_arr:\n",
    "            if n.id == e[0].id:\n",
    "                cnt_out += 1\n",
    "                out_ = e[1]\n",
    "            if n.id == e[1].id:\n",
    "                cnt_in += 1\n",
    "                in_ = e[0]\n",
    "        if cnt_in == 1 and cnt_out == 1 and out_.type == 0:\n",
    "            del_arr.append((n, in_, out_))\n",
    "    for d in del_arr:\n",
    "        g.remove_node(d[0])\n",
    "        g.add_edge(d[1], d[2])\n",
    "    return g\n",
    "\n",
    "\n",
    "def cut_line_node(g):\n",
    "    node_arr = list(g.nodes())\n",
    "\n",
    "    for n in node_arr:\n",
    "        edge_arr = list(g.edges())\n",
    "        cnt_in = 0\n",
    "        cnt_out = 0\n",
    "        for e in edge_arr:\n",
    "            if n.id == e[0].id:\n",
    "                cnt_out += 1\n",
    "                out_ = e[1]\n",
    "            if n.id == e[1].id:\n",
    "                cnt_in += 1\n",
    "                in_ = e[0]\n",
    "        if cnt_in == 1 and cnt_out == 1:\n",
    "            g.remove_node(n)\n",
    "            #             print \"remove\", n\n",
    "            g.add_edge(in_, out_)\n",
    "    #             print \"add_edge\", in_, out_\n",
    "    return g\n",
    "\n",
    "\n",
    "def get_seq_nodes(g):\n",
    "    res = []\n",
    "    node_arr = list(g.nodes())\n",
    "    for n in node_arr:\n",
    "        if n.type == 0:\n",
    "            res.append(copy.deepcopy(n))\n",
    "    return sorted(res, key=lambda x: x.id)\n",
    "\n",
    "\n",
    "def get_non_seq_nodes(g):\n",
    "    res = []\n",
    "    node_arr = list(g.nodes())\n",
    "    for n in node_arr:\n",
    "        if n.type != 0:\n",
    "            res.append(copy.deepcopy(n))\n",
    "    return sorted(res, key=lambda x: x.id)\n",
    "\n",
    "\n",
    "def get_all_text(g):\n",
    "    seq_arr = get_seq_nodes(g)\n",
    "    nonseq_arr = get_non_seq_nodes(g)\n",
    "    seq = [x.word for x in seq_arr]\n",
    "    nonseq = [x.word for x in nonseq_arr]\n",
    "    return seq + nonseq\n",
    "\n",
    "\n",
    "def get_all_id(g):\n",
    "    seq_arr = get_seq_nodes(g)\n",
    "    nonseq_arr = get_non_seq_nodes(g)\n",
    "    seq = [x.id for x in seq_arr]\n",
    "    nonseq = [x.id for x in nonseq_arr]\n",
    "    return seq + nonseq\n",
    "\n",
    "\n",
    "def get_id2word(g):\n",
    "    res = {}\n",
    "    seq_arr = get_seq_nodes(g)\n",
    "    nonseq_arr = get_non_seq_nodes(g)\n",
    "    for x in seq_arr:\n",
    "        res[x.id] = x.word\n",
    "    for x in nonseq_arr:\n",
    "        res[x.id] = x.word\n",
    "    return res\n",
    "\n",
    "\n",
    "def nodes_to_string(l):\n",
    "    return \" \".join([x.word for x in l])\n",
    "\n",
    "\n",
    "def print_edges(g):\n",
    "    edge_arr = list(g.edges())\n",
    "    for e in edge_arr:\n",
    "        print(e[0].word, e[1].word), (e[0].id, e[1].id)\n",
    "\n",
    "\n",
    "def print_nodes(g, he_ta=False):\n",
    "    nodes_arr = list(g.nodes())\n",
    "    if he_ta:\n",
    "        print([(n.word, n.id, n.head, n.tail) for n in nodes_arr])\n",
    "    else:\n",
    "        print([(n.word, n.id) for n in nodes_arr])\n",
    "\n",
    "\n",
    "def graph_connect(a_, b_):\n",
    "    a = copy.deepcopy(a_)\n",
    "    b = copy.deepcopy(b_)\n",
    "    max_id = 0\n",
    "    for n in a.nodes():\n",
    "        if n.id > max_id:\n",
    "            max_id = n.id\n",
    "    tmp = copy.deepcopy(b)\n",
    "    for n in tmp.nodes():\n",
    "        n.id += max_id\n",
    "\n",
    "    res = nx.union(a, tmp)\n",
    "    seq_nodes_arr = []\n",
    "    for n in res.nodes():\n",
    "        if n.type == 0:\n",
    "            seq_nodes_arr.append(n)\n",
    "    seq_nodes_arr.sort(key=lambda x: x.id)\n",
    "    for idx in range(len(seq_nodes_arr)):\n",
    "        if idx != len(seq_nodes_arr) - 1 and seq_nodes_arr[idx].tail == True:\n",
    "            if seq_nodes_arr[idx + 1].head == True:\n",
    "                res.add_edge(seq_nodes_arr[idx], seq_nodes_arr[idx + 1])\n",
    "                res.add_edge(seq_nodes_arr[idx + 1], seq_nodes_arr[idx])\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_vocab(g):\n",
    "    a = set()\n",
    "    for n in list(g.nodes()):\n",
    "        a.add(n.word)\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_adj(g):\n",
    "    # reverse the direction\n",
    "    adj_dict = {}\n",
    "    for node, n_dict in g.adjacency():\n",
    "        adj_dict[node.id] = []\n",
    "\n",
    "    for node, n_dict in g.adjacency():\n",
    "        for i in n_dict.items():\n",
    "            adj_dict[i[0].id].append(node.id)\n",
    "    return adj_dict\n",
    "\n",
    "\n",
    "def get_constituency_graph(input_tmp):\n",
    "    tmp_result = input_tmp\n",
    "\n",
    "    if _cut_root_node:\n",
    "        parse_str = cut_root_node(str(tmp_result['parse']))\n",
    "    else:\n",
    "        parse_str = str(tmp_result['parse'])\n",
    "    for punc in ['(', ')']:\n",
    "        parse_str = parse_str.replace(punc, ' ' + punc + ' ')\n",
    "    parse_list = str(parse_str).split()\n",
    "\n",
    "    res_graph = nx.DiGraph()\n",
    "    pstack = Stack()\n",
    "    idx = 0\n",
    "    while idx < len(parse_list):\n",
    "        if parse_list[idx] == '(':\n",
    "            new_node = Node(word=parse_list[idx + 1], id_=idx + 1, type_=1)\n",
    "            res_graph.add_node(new_node)\n",
    "            pstack.push(new_node)\n",
    "\n",
    "            if pstack.size() > 1:\n",
    "                node_2 = pstack.pop()\n",
    "                node_1 = pstack.pop()\n",
    "                res_graph.add_edge(node_1, node_2)\n",
    "                pstack.push(node_1)\n",
    "                pstack.push(node_2)\n",
    "        elif parse_list[idx] == ')':\n",
    "            pstack.pop()\n",
    "        elif parse_list[idx] in tmp_result['tok']:\n",
    "            new_node = Node(word=parse_list[idx], id_=idx, type_=0)\n",
    "            node_1 = pstack.pop()\n",
    "            if node_1.id != new_node.id:\n",
    "                res_graph.add_edge(node_1, new_node)\n",
    "            pstack.push(node_1)\n",
    "        idx += 1\n",
    "\n",
    "    max_id = 0\n",
    "    for n in res_graph.nodes():\n",
    "        if n.type == 0 and n.id > max_id:\n",
    "            max_id = n.id\n",
    "\n",
    "    min_id = 99999\n",
    "    for n in res_graph.nodes():\n",
    "        if n.type == 0 and n.id < min_id:\n",
    "            min_id = n.id\n",
    "\n",
    "    for n in res_graph.nodes():\n",
    "        if n.type == 0 and n.id == max_id:\n",
    "            n.tail = True\n",
    "        if n.type == 0 and n.id == min_id:\n",
    "            n.head = True\n",
    "    return res_graph\n",
    "\n",
    "\n",
    "def generate_batch_graph(output_file, string_batch):\n",
    "    # generate constituency graph\n",
    "    graph_list = []\n",
    "    processor = InputPreprocessor()\n",
    "    max_node_size = 0\n",
    "    for s in string_batch:\n",
    "\n",
    "        # generate multiple graph\n",
    "        if _split_sentence:\n",
    "            s_arr = split_str(s)\n",
    "\n",
    "            g = cut_line_node(get_constituency_graph(processor.featureExtract(s_arr[0])))\n",
    "            for sub_s in s_arr:\n",
    "                if sub_s != s_arr[0]:\n",
    "                    tmp = cut_line_node(get_constituency_graph(processor.featureExtract(sub_s)))\n",
    "                    g = graph_connect(g, tmp)\n",
    "\n",
    "        # decide how to cut nodes\n",
    "        if _cut_pos_node:\n",
    "            g = cut_pos_node(get_constituency_graph(processor.featureExtract(s)))\n",
    "        elif _cut_line_node:\n",
    "            g = cut_line_node(get_constituency_graph(processor.featureExtract(s)))\n",
    "        else:\n",
    "            g = (get_constituency_graph(processor.featureExtract(s)))\n",
    "\n",
    "        if len(list(g.nodes())) > max_node_size:\n",
    "            max_node_size = len(list(g.nodes()))\n",
    "        graph_list.append(g)\n",
    "\n",
    "    info_list = []\n",
    "    batch_size = len(string_batch)\n",
    "    for index in range(batch_size):\n",
    "        word_list = get_all_text(graph_list[index])\n",
    "        word_len = len(get_seq_nodes(graph_list[index]))\n",
    "        id_arr = get_all_id(graph_list[index])\n",
    "        adj_dic = get_adj(graph_list[index])\n",
    "        new_dic = {}\n",
    "\n",
    "        # transform id to position in wordlist\n",
    "        for k in adj_dic.keys():\n",
    "            new_dic[id_arr.index(k)] = [id_arr.index(x) for x in adj_dic[k]]\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        g_ids = {}\n",
    "        g_ids_features = {}\n",
    "        g_adj = {}\n",
    "\n",
    "        for idx in range(max_node_size):\n",
    "            g_ids[idx] = idx\n",
    "            if idx < len(word_list):\n",
    "                g_ids_features[idx] = word_list[idx]\n",
    "\n",
    "                if _link_word_nodes:\n",
    "                    if idx <= word_len - 1:\n",
    "                        if idx == 0:\n",
    "                            new_dic[idx].append(idx + 1)\n",
    "                        elif idx == word_len - 1:\n",
    "                            new_dic[idx].append(idx - 1)\n",
    "                        else:\n",
    "                            new_dic[idx].append(idx - 1)\n",
    "                            new_dic[idx].append(idx + 1)\n",
    "\n",
    "                g_adj[idx] = new_dic[idx]\n",
    "            else:\n",
    "                g_ids_features[idx] = '<P>'\n",
    "                g_adj[idx] = []\n",
    "\n",
    "        info['g_ids'] = g_ids\n",
    "        info['g_ids_features'] = g_ids_features\n",
    "        info['g_adj'] = g_adj\n",
    "\n",
    "        info_list.append(info)\n",
    "\n",
    "    with open(output_file, \"a+\") as f:\n",
    "        for idx in range(len(info_list)):\n",
    "            f.write(json.dumps(info_list[idx]) + '\\n')\n",
    "\n",
    "    batch_vocab = []\n",
    "    for x in graph_list:\n",
    "        non_arr = nodes_to_string(get_non_seq_nodes(x)).split()\n",
    "        for w in non_arr:\n",
    "            if w not in batch_vocab:\n",
    "                batch_vocab.append(w)\n",
    "    return batch_vocab\n",
    "\n",
    "\n",
    "def train_data_preprocess():\n",
    "    time_start = time.time()\n",
    "    word_manager = SymbolsManager(True)\n",
    "    word_manager.init_from_file(\"{}/vocab.q.txt\".format(source_data_dir), min_freq, max_vocab_size)\n",
    "    form_manager = SymbolsManager(True)\n",
    "    form_manager.init_from_file(\"{}/vocab.f.txt\".format(source_data_dir), 0, max_vocab_size)\n",
    "    print(word_manager.vocab_size)\n",
    "    print(form_manager.vocab_size)\n",
    "\n",
    "    data = []\n",
    "    with open(\"{}/{}.txt\".format(source_data_dir, \"train\"), \"r\") as f:\n",
    "        for line in f:\n",
    "            l_list = line.split(\"\\t\")\n",
    "            w_list = l_list[0].strip().split(' ')\n",
    "            r_list = form_manager.get_symbol_idx_for_list(l_list[1].strip().split(' '))\n",
    "            cur_tree = convert_to_tree(r_list, 0, len(r_list), form_manager)\n",
    "\n",
    "            data.append((w_list, r_list, cur_tree))\n",
    "\n",
    "    out_graphfile = \"{}/graph.train\".format(output_data_dir)\n",
    "    if os.path.exists(out_graphfile):\n",
    "        os.remove(out_graphfile)\n",
    "    # generate batch graph here\n",
    "    if len(data) % batch_size != 0:\n",
    "        n = len(data)\n",
    "        for i in range(batch_size - len(data) % batch_size):\n",
    "            data.insert(n - i - 1, copy.deepcopy(data[n - i - 1]))\n",
    "\n",
    "    index = 0\n",
    "    while index + batch_size <= len(data):\n",
    "        # generate graphs with order and dependency information\n",
    "        input_batch = [\" \".join(data[index + idx][0]) for idx in range(batch_size)]\n",
    "        new_vocab = generate_batch_graph(output_file=out_graphfile, string_batch=input_batch)\n",
    "        for w in new_vocab:\n",
    "            if w not in word_manager.symbol2idx:\n",
    "                word_manager.add_symbol(w)\n",
    "                print(\"{} Added.\".format(w))\n",
    "        index += batch_size\n",
    "        print(index)\n",
    "\n",
    "    out_datafile = \"{}/train.pkl\".format(output_data_dir)\n",
    "    with open(out_datafile, \"wb\") as out_data:\n",
    "        pkl.dump(data, out_data)\n",
    "\n",
    "    out_mapfile = \"{}/map.pkl\".format(output_data_dir)\n",
    "    with open(out_mapfile, \"wb\") as out_map:\n",
    "        pkl.dump([word_manager, form_manager], out_map)\n",
    "\n",
    "    print(word_manager.vocab_size)\n",
    "    print(form_manager.vocab_size)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print(\"time used:\" + str(time_end - time_start))\n",
    "\n",
    "def test_data_preprocess():\n",
    "    data = []\n",
    "    managers = pkl.load(open(\"{}/map.pkl\".format(output_data_dir), \"rb\"))\n",
    "    word_manager, form_manager = managers\n",
    "    with open(\"{}/{}.txt\".format(source_data_dir, \"all_aqua\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            l_list = line.split(\"\\t\")\n",
    "            w_list = l_list[0].strip().split(' ')\n",
    "            # r_list = form_manager.get_symbol_idx_for_list(l_list[1].strip().split(' '))\n",
    "            eq_temp = \"x=x+1\"\n",
    "            r_list = form_manager.get_symbol_idx_for_list(eq_temp.strip().split(' '))\n",
    "            cur_tree = convert_to_tree(r_list, 0, len(r_list), form_manager)\n",
    "            data.append((w_list, r_list, cur_tree))\n",
    "    out_datafile = \"{}/test.pkl\".format(output_data_dir)\n",
    "    with open(out_datafile, \"wb\") as out_data:\n",
    "        pkl.dump(data, out_data)\n",
    "\n",
    "    out_graphfile = \"{}/graph.test\".format(output_data_dir)\n",
    "    if os.path.exists(out_graphfile):\n",
    "        os.remove(out_graphfile)\n",
    "\n",
    "    index = 0\n",
    "    while index + batch_size <= len(data):\n",
    "        # generate graphs with order and dependency information\n",
    "        input_batch = [\" \".join(data[index + idx][0]) for idx in range(batch_size)]\n",
    "        new_vocab = generate_batch_graph(output_file=out_graphfile, string_batch=input_batch)\n",
    "        index += batch_size\n",
    "\n",
    "    if index != len(data):\n",
    "        input_batch = [\" \".join(data[idx][0]) for idx in range(index, len(data))]\n",
    "        new_vocab = generate_batch_graph(output_file=out_graphfile, string_batch=input_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
