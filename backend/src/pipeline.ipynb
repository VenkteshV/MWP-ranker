{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "\n",
    "path.append(r'../../src')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import tqdm\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from graph2tree import *\n",
    "import networkx as nx\n",
    "import sys\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from data_utils import convert_to_tree\n",
    "from pythonds.basic.stack import Stack\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1ca9226c930>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cut_root_node = True\n",
    "_cut_line_node = True\n",
    "_cut_pos_node = False\n",
    "_link_word_nodes = False\n",
    "_split_sentence = False\n",
    "\n",
    "source_data_dir = \"../data/TextData/\"\n",
    "output_data_dir = \"../data/GraphConstruction/\"\n",
    "batch_size = 30\n",
    "min_freq = 2\n",
    "max_vocab_size = 15000\n",
    "seed = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test data preprocess\n",
    "class InputPreprocessor(object):\n",
    "    def __init__(self, url='http://localhost:9000'):\n",
    "        self.nlp = StanfordCoreNLP(url)\n",
    "\n",
    "    def featureExtract(self, src_text, whiteSpace=True):\n",
    "        if src_text.strip() in preparsed_file.keys():\n",
    "            return preparsed_file[src_text.strip()]\n",
    "        print(\"miss!\")\n",
    "        data = {}\n",
    "        output = self.nlp.annotate(src_text.strip(), properties={\n",
    "            'annotators': \"tokenize,ssplit,pos,parse\",\n",
    "            \"tokenize.options\": \"splitHyphenated=true,normalizeParentheses=false\",\n",
    "            \"tokenize.whitespace\": whiteSpace,\n",
    "            'ssplit.isOneSentence': True,\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "\n",
    "        snt = output['sentences'][0][\"tokens\"]\n",
    "        depency = output['sentences'][0][\"basicDependencies\"]\n",
    "        data[\"tok\"] = []\n",
    "        data[\"pos\"] = []\n",
    "        data[\"dep\"] = []\n",
    "        data[\"governor\"] = []\n",
    "        data[\"dependent\"] = []\n",
    "        data['parse'] = output['sentences'][0]['parse']\n",
    "        for snt_tok in snt:\n",
    "            data[\"tok\"].append(snt_tok['word'])\n",
    "            data[\"pos\"].append(snt_tok['pos'])\n",
    "        for deps in depency:\n",
    "            data[\"dep\"].append(deps['dep'])\n",
    "            data[\"governor\"].append(deps['governor'])\n",
    "            data[\"dependent\"].append(deps['dependent'])\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_preparsed_file():\n",
    "    if not os.path.exists(output_data_dir+\"/file_for_parsing.pkl\"):\n",
    "        file_for_parsing = {}\n",
    "        processor_tmp = InputPreprocessor()\n",
    "\n",
    "        with open(source_data_dir + \"all.txt\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for l in tqdm.tqdm(lines):\n",
    "                str_ = l.strip().split('\\t')[0]\n",
    "                file_for_parsing[str_] = processor_tmp.featureExtract(str_)\n",
    "\n",
    "        pkl.dump(file_for_parsing, open(output_data_dir+\"./file_for_parsing.pkl\", \"wb\"))\n",
    "    else:\n",
    "        preparsed_file = pkl.load(open(output_data_dir+\"./file_for_parsing.pkl\", \"rb\"))\n",
    "    return preparsed_file\n",
    "\n",
    "\n",
    "preparsed_file = get_preparsed_file()\n",
    "\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, word, type_, id_):\n",
    "        # word: this node's text\n",
    "        self.word = word\n",
    "\n",
    "        # type=0 for word nodes, 1 for constituency nodes, 2 for dependency nodes(if they exist)\n",
    "        self.type = type_\n",
    "\n",
    "        # id: unique identifier for every node\n",
    "        self.id = id_\n",
    "\n",
    "        self.head = False\n",
    "\n",
    "        self.tail = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "\n",
    "\n",
    "def split_str(string):\n",
    "    if \" . \" not in string:\n",
    "        return [string]\n",
    "    else:\n",
    "        s_arr = string.split(\" . \")\n",
    "        res = []\n",
    "        for s in s_arr:\n",
    "            if s[-1] != \".\" and s != s_arr[-1]:\n",
    "                s = s + \" .\"\n",
    "            res.append(s)\n",
    "        return res\n",
    "\n",
    "\n",
    "def cut_root_node(con_string):\n",
    "    tmp = con_string\n",
    "    if con_string[0] == '(' and con_string[-1] == ')':\n",
    "        tmp = con_string[1:-1].replace(\"ROOT\", \"\")\n",
    "        if tmp[0] == '\\n':\n",
    "            tmp = tmp[1:]\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def cut_pos_node(g):\n",
    "    node_arr = list(g.nodes())\n",
    "    del_arr = []\n",
    "    for n in node_arr:\n",
    "        edge_arr = list(g.edges())\n",
    "        cnt_in = 0\n",
    "        cnt_out = 0\n",
    "        for e in edge_arr:\n",
    "            if n.id == e[0].id:\n",
    "                cnt_out += 1\n",
    "                out_ = e[1]\n",
    "            if n.id == e[1].id:\n",
    "                cnt_in += 1\n",
    "                in_ = e[0]\n",
    "        if cnt_in == 1 and cnt_out == 1 and out_.type == 0:\n",
    "            del_arr.append((n, in_, out_))\n",
    "    for d in del_arr:\n",
    "        g.remove_node(d[0])\n",
    "        g.add_edge(d[1], d[2])\n",
    "    return g\n",
    "\n",
    "\n",
    "def cut_line_node(g):\n",
    "    node_arr = list(g.nodes())\n",
    "\n",
    "    for n in node_arr:\n",
    "        edge_arr = list(g.edges())\n",
    "        cnt_in = 0\n",
    "        cnt_out = 0\n",
    "        for e in edge_arr:\n",
    "            if n.id == e[0].id:\n",
    "                cnt_out += 1\n",
    "                out_ = e[1]\n",
    "            if n.id == e[1].id:\n",
    "                cnt_in += 1\n",
    "                in_ = e[0]\n",
    "        if cnt_in == 1 and cnt_out == 1:\n",
    "            g.remove_node(n)\n",
    "            #             print \"remove\", n\n",
    "            g.add_edge(in_, out_)\n",
    "    #             print \"add_edge\", in_, out_\n",
    "    return g\n",
    "\n",
    "\n",
    "def get_seq_nodes(g):\n",
    "    res = []\n",
    "    node_arr = list(g.nodes())\n",
    "    for n in node_arr:\n",
    "        if n.type == 0:\n",
    "            res.append(copy.deepcopy(n))\n",
    "    return sorted(res, key=lambda x: x.id)\n",
    "\n",
    "\n",
    "def get_non_seq_nodes(g):\n",
    "    res = []\n",
    "    node_arr = list(g.nodes())\n",
    "    for n in node_arr:\n",
    "        if n.type != 0:\n",
    "            res.append(copy.deepcopy(n))\n",
    "    return sorted(res, key=lambda x: x.id)\n",
    "\n",
    "\n",
    "def get_all_text(g):\n",
    "    seq_arr = get_seq_nodes(g)\n",
    "    nonseq_arr = get_non_seq_nodes(g)\n",
    "    seq = [x.word for x in seq_arr]\n",
    "    nonseq = [x.word for x in nonseq_arr]\n",
    "    return seq + nonseq\n",
    "\n",
    "\n",
    "def get_all_id(g):\n",
    "    seq_arr = get_seq_nodes(g)\n",
    "    nonseq_arr = get_non_seq_nodes(g)\n",
    "    seq = [x.id for x in seq_arr]\n",
    "    nonseq = [x.id for x in nonseq_arr]\n",
    "    return seq + nonseq\n",
    "\n",
    "\n",
    "def get_id2word(g):\n",
    "    res = {}\n",
    "    seq_arr = get_seq_nodes(g)\n",
    "    nonseq_arr = get_non_seq_nodes(g)\n",
    "    for x in seq_arr:\n",
    "        res[x.id] = x.word\n",
    "    for x in nonseq_arr:\n",
    "        res[x.id] = x.word\n",
    "    return res\n",
    "\n",
    "\n",
    "def nodes_to_string(l):\n",
    "    return \" \".join([x.word for x in l])\n",
    "\n",
    "\n",
    "def print_edges(g):\n",
    "    edge_arr = list(g.edges())\n",
    "    for e in edge_arr:\n",
    "        print(e[0].word, e[1].word), (e[0].id, e[1].id)\n",
    "\n",
    "\n",
    "def print_nodes(g, he_ta=False):\n",
    "    nodes_arr = list(g.nodes())\n",
    "    if he_ta:\n",
    "        print([(n.word, n.id, n.head, n.tail) for n in nodes_arr])\n",
    "    else:\n",
    "        print([(n.word, n.id) for n in nodes_arr])\n",
    "\n",
    "\n",
    "def graph_connect(a_, b_):\n",
    "    a = copy.deepcopy(a_)\n",
    "    b = copy.deepcopy(b_)\n",
    "    max_id = 0\n",
    "    for n in a.nodes():\n",
    "        if n.id > max_id:\n",
    "            max_id = n.id\n",
    "    tmp = copy.deepcopy(b)\n",
    "    for n in tmp.nodes():\n",
    "        n.id += max_id\n",
    "\n",
    "    res = nx.union(a, tmp)\n",
    "    seq_nodes_arr = []\n",
    "    for n in res.nodes():\n",
    "        if n.type == 0:\n",
    "            seq_nodes_arr.append(n)\n",
    "    seq_nodes_arr.sort(key=lambda x: x.id)\n",
    "    for idx in range(len(seq_nodes_arr)):\n",
    "        if idx != len(seq_nodes_arr) - 1 and seq_nodes_arr[idx].tail == True:\n",
    "            if seq_nodes_arr[idx + 1].head == True:\n",
    "                res.add_edge(seq_nodes_arr[idx], seq_nodes_arr[idx + 1])\n",
    "                res.add_edge(seq_nodes_arr[idx + 1], seq_nodes_arr[idx])\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_vocab(g):\n",
    "    a = set()\n",
    "    for n in list(g.nodes()):\n",
    "        a.add(n.word)\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_adj(g):\n",
    "    # reverse the direction\n",
    "    adj_dict = {}\n",
    "    for node, n_dict in g.adjacency():\n",
    "        adj_dict[node.id] = []\n",
    "\n",
    "    for node, n_dict in g.adjacency():\n",
    "        for i in n_dict.items():\n",
    "            adj_dict[i[0].id].append(node.id)\n",
    "    return adj_dict\n",
    "\n",
    "\n",
    "def get_constituency_graph(input_tmp):\n",
    "    tmp_result = input_tmp\n",
    "\n",
    "    if _cut_root_node:\n",
    "        parse_str = cut_root_node(str(tmp_result['parse']))\n",
    "    else:\n",
    "        parse_str = str(tmp_result['parse'])\n",
    "    for punc in ['(', ')']:\n",
    "        parse_str = parse_str.replace(punc, ' ' + punc + ' ')\n",
    "    parse_list = str(parse_str).split()\n",
    "\n",
    "    res_graph = nx.DiGraph()\n",
    "    pstack = Stack()\n",
    "    idx = 0\n",
    "    while idx < len(parse_list):\n",
    "        if parse_list[idx] == '(':\n",
    "            new_node = Node(word=parse_list[idx + 1], id_=idx + 1, type_=1)\n",
    "            res_graph.add_node(new_node)\n",
    "            pstack.push(new_node)\n",
    "\n",
    "            if pstack.size() > 1:\n",
    "                node_2 = pstack.pop()\n",
    "                node_1 = pstack.pop()\n",
    "                res_graph.add_edge(node_1, node_2)\n",
    "                pstack.push(node_1)\n",
    "                pstack.push(node_2)\n",
    "        elif parse_list[idx] == ')':\n",
    "            pstack.pop()\n",
    "        elif parse_list[idx] in tmp_result['tok']:\n",
    "            new_node = Node(word=parse_list[idx], id_=idx, type_=0)\n",
    "            node_1 = pstack.pop()\n",
    "            if node_1.id != new_node.id:\n",
    "                res_graph.add_edge(node_1, new_node)\n",
    "            pstack.push(node_1)\n",
    "        idx += 1\n",
    "\n",
    "    max_id = 0\n",
    "    for n in res_graph.nodes():\n",
    "        if n.type == 0 and n.id > max_id:\n",
    "            max_id = n.id\n",
    "\n",
    "    min_id = 99999\n",
    "    for n in res_graph.nodes():\n",
    "        if n.type == 0 and n.id < min_id:\n",
    "            min_id = n.id\n",
    "\n",
    "    for n in res_graph.nodes():\n",
    "        if n.type == 0 and n.id == max_id:\n",
    "            n.tail = True\n",
    "        if n.type == 0 and n.id == min_id:\n",
    "            n.head = True\n",
    "    return res_graph\n",
    "\n",
    "\n",
    "def generate_batch_graph(string_batch):\n",
    "    # generate constituency graph\n",
    "    graph_list = []\n",
    "    processor = InputPreprocessor()\n",
    "    max_node_size = 0\n",
    "    for s in string_batch:\n",
    "\n",
    "        # generate multiple graph\n",
    "        if _split_sentence:\n",
    "            s_arr = split_str(s)\n",
    "\n",
    "            g = cut_line_node(get_constituency_graph(processor.featureExtract(s_arr[0])))\n",
    "            for sub_s in s_arr:\n",
    "                if sub_s != s_arr[0]:\n",
    "                    tmp = cut_line_node(get_constituency_graph(processor.featureExtract(sub_s)))\n",
    "                    g = graph_connect(g, tmp)\n",
    "\n",
    "        # decide how to cut nodes\n",
    "        if _cut_pos_node:\n",
    "            g = cut_pos_node(get_constituency_graph(processor.featureExtract(s)))\n",
    "        elif _cut_line_node:\n",
    "            g = cut_line_node(get_constituency_graph(processor.featureExtract(s)))\n",
    "        else:\n",
    "            g = (get_constituency_graph(processor.featureExtract(s)))\n",
    "\n",
    "        if len(list(g.nodes())) > max_node_size:\n",
    "            max_node_size = len(list(g.nodes()))\n",
    "        graph_list.append(g)\n",
    "\n",
    "    info_list = []\n",
    "    batch_size = len(string_batch)\n",
    "    for index in range(batch_size):\n",
    "        word_list = get_all_text(graph_list[index])\n",
    "        word_len = len(get_seq_nodes(graph_list[index]))\n",
    "        id_arr = get_all_id(graph_list[index])\n",
    "        adj_dic = get_adj(graph_list[index])\n",
    "        new_dic = {}\n",
    "\n",
    "        # transform id to position in wordlist\n",
    "        for k in adj_dic.keys():\n",
    "            new_dic[id_arr.index(k)] = [id_arr.index(x) for x in adj_dic[k]]\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        g_ids = {}\n",
    "        g_ids_features = {}\n",
    "        g_adj = {}\n",
    "\n",
    "        for idx in range(max_node_size):\n",
    "            g_ids[idx] = idx\n",
    "            if idx < len(word_list):\n",
    "                g_ids_features[idx] = word_list[idx]\n",
    "\n",
    "                if _link_word_nodes:\n",
    "                    if idx <= word_len - 1:\n",
    "                        if idx == 0:\n",
    "                            new_dic[idx].append(idx + 1)\n",
    "                        elif idx == word_len - 1:\n",
    "                            new_dic[idx].append(idx - 1)\n",
    "                        else:\n",
    "                            new_dic[idx].append(idx - 1)\n",
    "                            new_dic[idx].append(idx + 1)\n",
    "\n",
    "                g_adj[idx] = new_dic[idx]\n",
    "            else:\n",
    "                g_ids_features[idx] = '<P>'\n",
    "                g_adj[idx] = []\n",
    "\n",
    "        info['g_ids'] = g_ids\n",
    "        info['g_ids_features'] = g_ids_features\n",
    "        info['g_adj'] = g_adj\n",
    "\n",
    "        info_list.append(info)\n",
    "\n",
    "    # with open(output_file, \"a+\") as f:\n",
    "        return info_list[0]\n",
    "\n",
    "    batch_vocab = []\n",
    "    for x in graph_list:\n",
    "        non_arr = nodes_to_string(get_non_seq_nodes(x)).split()\n",
    "        for w in non_arr:\n",
    "            if w not in batch_vocab:\n",
    "                batch_vocab.append(w)\n",
    "    return batch_vocab\n",
    "\n",
    "\n",
    "def test_data_preprocess(line):\n",
    "    data = []\n",
    "    managers = pkl.load(open(\"{}/map.pkl\".format(output_data_dir), \"rb\"))\n",
    "    word_manager, form_manager = managers\n",
    "    l_list = line.split(\"\\t\")\n",
    "    w_list = l_list[0].strip().split(' ')\n",
    "    r_list = form_manager.get_symbol_idx_for_list(l_list[1].strip().split(' '))\n",
    "    cur_tree = convert_to_tree(r_list, 0, len(r_list), form_manager)\n",
    "    data.append((w_list, r_list, cur_tree))\n",
    "    index = 0\n",
    "    if index != len(data):\n",
    "        input_batch = [\" \".join(data[idx][0]) for idx in range(index, len(data))]\n",
    "        result = generate_batch_graph(string_batch=input_batch)\n",
    "\n",
    "    return [data[0], result]\n",
    "\n",
    "if  len(sys.argv) != 2:\n",
    "    test_data_preprocess(\"a bee has 2 legs . how many legs do 1 bees have ?\tx = ( 2 * 1 )\")\n",
    "else:\n",
    "    test_data_preprocess(sys.argv[1]+'\\t'+'x = ( 2 * 1 )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_batch_graph(graphs):\n",
    "    g_ids = {}\n",
    "    g_ids_features = {}\n",
    "    g_fw_adj = {}\n",
    "    g_bw_adj = {}\n",
    "    g_nodes = []\n",
    "    for g in graphs:\n",
    "        ids = g['g_ids']\n",
    "        id_adj = g['g_adj']\n",
    "        features = g['g_ids_features']\n",
    "        nodes = []\n",
    "\n",
    "        # we first add all nodes into batch_graph and create a mapping from graph id to batch_graph id, this mapping will be\n",
    "        # used in the creation of fw_adj and bw_adj\n",
    "\n",
    "        id_gid_map = {}\n",
    "        offset = len(g_ids.keys())\n",
    "        for id in ids:\n",
    "            id = int(id)\n",
    "            g_ids[offset + id] = len(g_ids.keys())\n",
    "            g_ids_features[offset + id] = features[id]\n",
    "            id_gid_map[id] = offset + id\n",
    "            nodes.append(offset + id)\n",
    "        g_nodes.append(nodes)\n",
    "\n",
    "        for id in id_adj:\n",
    "            adj = id_adj[id]\n",
    "            id = int(id)\n",
    "            g_id = id_gid_map[id]\n",
    "            if g_id not in g_fw_adj:\n",
    "                g_fw_adj[g_id] = []\n",
    "            for t in adj:\n",
    "                t = int(t)\n",
    "                g_t = id_gid_map[t]\n",
    "                g_fw_adj[g_id].append(g_t)\n",
    "                if g_t not in g_bw_adj:\n",
    "                    g_bw_adj[g_t] = []\n",
    "                g_bw_adj[g_t].append(g_id)\n",
    "\n",
    "    node_size = len(g_ids.keys())\n",
    "    for id in range(node_size):\n",
    "        if id not in g_fw_adj:\n",
    "            g_fw_adj[id] = []\n",
    "        if id not in g_bw_adj:\n",
    "            g_bw_adj[id] = []\n",
    "\n",
    "    graph = {}\n",
    "    graph['g_ids'] = g_ids\n",
    "    graph['g_ids_features'] = g_ids_features\n",
    "    graph['g_nodes'] = g_nodes\n",
    "    graph['g_fw_adj'] = g_fw_adj\n",
    "    graph['g_bw_adj'] = g_bw_adj\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from graph2tree import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "checkpoint = torch.load(r'checkpoint_dir/output_model')\n",
    "encoder = checkpoint[\"encoder\"]\n",
    "decoder = checkpoint[\"decoder\"]\n",
    "attention_decoder = checkpoint[\"attention_decoder\"]\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "attention_decoder.eval()\n",
    "\n",
    "managers = pkl.load( open(r\"../data/GraphConstruction/map.pkl\", \"rb\" ) )\n",
    "word_manager, form_manager = managers\n",
    "\n",
    "# data = pkl.load(open(\"{}/test.pkl\".format(args.data_dir), \"rb\"))\n",
    "# graph_test_list = graph_utils.read_graph_data(\"{}/graph.test\".format(args.data_dir))\n",
    "\n",
    "reference_list = []\n",
    "candidate_list = []\n",
    "# save as pandas dataframe\n",
    "def g2tree(eqaution, graph_info):\n",
    "    x = eqaution\n",
    "    reference = x[1]\n",
    "    graph_batch = cons_batch_graph([graph_info])\n",
    "    graph_input = graph_utils.vectorize_batch_graph(graph_batch, word_manager)\n",
    "\n",
    "    candidate = do_generate(encoder, decoder, attention_decoder, graph_input, word_manager, form_manager, '', True, checkpoint)\n",
    "    candidate = [int(c) for c in candidate]\n",
    "    ans = ' '.join([form_manager.idx2symbol[int(c)] for c in candidate])\n",
    "    # print(\"Question: \", data[i])\n",
    "    num_left_paren = sum(1 for c in candidate if form_manager.idx2symbol[int(c)]== \"(\")\n",
    "    num_right_paren = sum(1 for c in candidate if form_manager.idx2symbol[int(c)]== \")\")\n",
    "    diff = num_left_paren - num_right_paren\n",
    "\n",
    "    if diff > 0:\n",
    "        for i in range(diff):\n",
    "            candidate.append(form_manager.symbol2idx[\")\"])\n",
    "    elif diff < 0:\n",
    "        candidate = candidate[:diff]\n",
    "    ref_str = convert_to_string(reference, form_manager)\n",
    "    cand_str = convert_to_string(candidate, form_manager)\n",
    "\n",
    "    reference_list.append(reference)\n",
    "    candidate_list.append(candidate)\n",
    "    print(ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph\n",
    "from comparator_tree import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        ques                   eqn  \\\n",
      "0                         what is 1 % of 2 ?  x = ( 1 * 0.01 ) * 2   \n",
      "1                         what is 8 % of 2 ?  x = ( 8 * 0.01 ) * 2   \n",
      "2                  2 is 1 % of what number ?  2 = ( 1 * 0.01 ) * x   \n",
      "3            what is divided by 1 to get 2 ?             x / 1 = 2   \n",
      "4  2 of a number is 1 , what is the number ?             2 * x = 1   \n",
      "\n",
      "                                                tree  \n",
      "0  [<comparator_tree.nptr object at 0x000001CAB3F...  \n",
      "1  [<comparator_tree.nptr object at 0x000001CAB3F...  \n",
      "2  [<comparator_tree.nptr object at 0x000001CAB3F...  \n",
      "3  [<comparator_tree.nptr object at 0x000001CAB3F...  \n",
      "4  [<comparator_tree.nptr object at 0x000001CAB3F...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../data/TextData/all_df.pkl')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "tree = df['tree']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mques\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[43mtmp\u001B[49m],\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m, df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meqn\u001B[39m\u001B[38;5;124m'\u001B[39m][tmp])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "print(df['ques'][tmp],'\\n', df['eqn'][tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = test_data_preprocess(\"there are 2 baskets . there are 1 apples in each basket . how many apples are there in all ?\tx = ( 2 * 1 )\")\n",
    "eqn = g2tree(res[0], res[1])\n",
    "print(eqn)\n",
    "eqn_parsed = graph.eq_parser(eqn)\n",
    "tmp = []\n",
    "for j in range(len(tree)):\n",
    "    if len(tree[j]) == 0:\n",
    "        continue\n",
    "    if compare_tree(eqn_parsed[0], tree[j][0]) and compare_tree(eqn_parsed[1], tree[j][1]):\n",
    "        tmp.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/data')\n",
    "def hello_world():\n",
    "    return {'ans-list':'Hello Sammy!'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:1235\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "app.run(port=1235)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtmp\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "tmp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
